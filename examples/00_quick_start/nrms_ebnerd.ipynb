{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we illustrate how to use the Neural News Recommendation with Multi-Head Self-Attention ([NRMS](https://aclanthology.org/D19-1671/)). The implementation is taken from the [recommenders](https://github.com/recommenders-team/recommenders) repository. We have simply stripped the model to keep it cleaner.\n",
    "\n",
    "We use a small dataset, which is downloaded from [recsys.eb.dk](https://recsys.eb.dk/). All the datasets are stored in the folder path ```~/ebnerd_data/*```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "from ebrec.models.newsrec_pytorch.dataloader import NRMSDataLoader\n",
    "from ebrec.models.newsrec_pytorch.model_config import hparams_nrms\n",
    "from ebrec.models.newsrec_pytorch import NRMSModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate labels\n",
    "We sample a few just to get started. For testset we just make up a dummy column with 0 and 1 - this is not the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\ebnerd_data\n"
     ]
    }
   ],
   "source": [
    "PATH = Path(\"~/ebnerd_data\").expanduser()\n",
    "print(PATH)\n",
    "DATASPLIT = \"ebnerd_demo\"\n",
    "DUMP_DIR = PATH.joinpath(\"downloads1\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we sample the dataset, just to keep it smaller. Also, one can simply add the testset similary to the validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>list[i8]</td></tr></thead><tbody><tr><td>202953</td><td>[9767620, 9768707, … 9768829]</td><td>[9695098, 9779498, … 9695098]</td><td>[9779498]</td><td>322980416</td><td>[0, 1, … 0]</td></tr><tr><td>383378</td><td>[9770400, 9770594, … 9770798]</td><td>[9778628, 9777034, … 9777492]</td><td>[9777034]</td><td>130055612</td><td>[0, 1, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 6)\n",
       "┌─────────┬───────────────────┬───────────────────┬──────────────────┬───────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_fixed  ┆ article_ids_invie ┆ article_ids_clic ┆ impression_id ┆ labels      │\n",
       "│ ---     ┆ ---               ┆ w                 ┆ ked              ┆ ---           ┆ ---         │\n",
       "│ u32     ┆ list[i32]         ┆ ---               ┆ ---              ┆ u32           ┆ list[i8]    │\n",
       "│         ┆                   ┆ list[i64]         ┆ list[i64]        ┆               ┆             │\n",
       "╞═════════╪═══════════════════╪═══════════════════╪══════════════════╪═══════════════╪═════════════╡\n",
       "│ 202953  ┆ [9767620,         ┆ [9695098,         ┆ [9779498]        ┆ 322980416     ┆ [0, 1, … 0] │\n",
       "│         ┆ 9768707, …        ┆ 9779498, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9768829]          ┆ 9695098]          ┆                  ┆               ┆             │\n",
       "│ 383378  ┆ [9770400,         ┆ [9778628,         ┆ [9777034]        ┆ 130055612     ┆ [0, 1, … 0] │\n",
       "│         ┆ 9770594, …        ┆ 9777034, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9770798]          ┆ 9777492]          ┆                  ┆               ┆             │\n",
       "└─────────┴───────────────────┴───────────────────┴──────────────────┴───────────────┴─────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "]\n",
    "HISTORY_SIZE = 10\n",
    "FRACTION = 0.01\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3037230</td><td>&quot;Ishockey-spill…</td><td>&quot;ISHOCKEY: Isho…</td><td>2023-06-29 06:20:57</td><td>false</td><td>&quot;Ambitionerne o…</td><td>2003-08-28 08:55:00</td><td>null</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Kendt&quot;, … &quot;Mindre ulykke&quot;]</td><td>142</td><td>[327, 334]</td><td>&quot;sport&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9752</td><td>&quot;Negative&quot;</td></tr><tr><td>3044020</td><td>&quot;Prins Harry tv…</td><td>&quot;Hoffet tvang P…</td><td>2023-06-29 06:21:16</td><td>false</td><td>&quot;Den britiske t…</td><td>2005-06-29 08:47:00</td><td>[3097307, 3097197, 3104927]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[&quot;Harry&quot;, &quot;James Hewitt&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Kendt&quot;, … &quot;Personfarlig kriminalitet&quot;]</td><td>414</td><td>[432]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.7084</td><td>&quot;Negative&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3037230   ┆ Ishockey- ┆ ISHOCKEY: ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9752    ┆ Negative │\n",
       "│           ┆ spiller:  ┆ Ishockey- ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ Jeg       ┆ spilleren ┆ 06:20:57  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ troede    ┆ Seb…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ jeg…      ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3044020   ┆ Prins     ┆ Hoffet    ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.7084    ┆ Negative │\n",
       "│           ┆ Harry     ┆ tvang     ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tvunget   ┆ Prins     ┆ 06:21:16  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ til       ┆ Harry til ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ dna-test  ┆ at …      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(\"ebnerd_demo/articles.parquet\"))\n",
    "df_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model using HuggingFace's tokenizer and wordembedding\n",
    "In the original implementation, they use the GloVe embeddings and tokenizer. To get going fast, we'll use a multilingual LLM from Hugging Face. \n",
    "Utilizing the tokenizer to tokenize the articles and the word-embedding to init NRMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=32,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using device: cpu\n",
      "torch.Size([250002, 768])\n",
      "News encoder:  NewsEncoder(\n",
      "  (embedding): Embedding(250002, 768)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (multihead_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (attention): AttLayer2()\n",
      ")\n",
      "User encoder:  UserEncoder(\n",
      "  (news_encoder): NewsEncoder(\n",
      "    (embedding): Embedding(250002, 768)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (multihead_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (attention): AttLayer2()\n",
      "  )\n",
      "  (multihead_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (additive_attention): AttLayer2()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "NRMSModel(\n",
      "  (news_encoder): NewsEncoder(\n",
      "    (embedding): Embedding(250002, 768)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (multihead_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (attention): AttLayer2()\n",
      "  )\n",
      "  (candidate_encoder): NewsEncoder(\n",
      "    (embedding): Embedding(250002, 768)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (multihead_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (attention): AttLayer2()\n",
      "  )\n",
      "  (user_encoder): UserEncoder(\n",
      "    (news_encoder): NewsEncoder(\n",
      "      (embedding): Embedding(250002, 768)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (multihead_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (attention): AttLayer2()\n",
      "    )\n",
      "    (multihead_attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (additive_attention): AttLayer2()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_14804\\4166849013.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  his_input_title = torch.tensor(his_input_title, dtype=torch.long).to(device)\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_14804\\4166849013.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred_input_title = torch.tensor(pred_input_title, dtype=torch.long).to(device)\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_14804\\4166849013.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape for user encoder: torch.Size([32, 10, 30])\n",
      "Input shape for news encoder: torch.Size([32, 5, 30])\n",
      "embedded sequence size:  torch.Size([320, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([320, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([320, 768])\n",
      "Click title presents:  torch.Size([32, 10, 768])\n",
      "Multihead attention output from user encoder:  torch.Size([32, 10, 768])\n",
      "User encoder output after additive:  torch.Size([32, 768])\n",
      "embedded sequence size:  torch.Size([160, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([160, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([160, 768])\n",
      "User present shape:  torch.Size([32, 1, 768])\n",
      "News present shape:  torch.Size([32, 5, 768])\n",
      "Preds after fot shape:  torch.Size([32, 5])\n",
      "Preds shape:  torch.Size([32, 5])\n",
      "tensor([[0.1998, 0.2003, 0.2006, 0.1997, 0.1996],\n",
      "        [0.2001, 0.1998, 0.1995, 0.2009, 0.1997],\n",
      "        [0.2004, 0.1996, 0.1995, 0.1998, 0.2007],\n",
      "        [0.2008, 0.1993, 0.1992, 0.1994, 0.2013],\n",
      "        [0.1997, 0.2006, 0.1998, 0.2001, 0.1998],\n",
      "        [0.2002, 0.1992, 0.1995, 0.2007, 0.2004],\n",
      "        [0.1996, 0.1993, 0.2006, 0.1999, 0.2006],\n",
      "        [0.2000, 0.1985, 0.2006, 0.2007, 0.2002],\n",
      "        [0.2000, 0.2009, 0.1988, 0.2005, 0.1997],\n",
      "        [0.1991, 0.2003, 0.2005, 0.1997, 0.2005],\n",
      "        [0.2006, 0.1992, 0.1997, 0.2002, 0.2003],\n",
      "        [0.2001, 0.2007, 0.2003, 0.1991, 0.1998],\n",
      "        [0.1995, 0.1994, 0.1999, 0.2014, 0.1997],\n",
      "        [0.1996, 0.1994, 0.1997, 0.2006, 0.2006],\n",
      "        [0.2010, 0.1997, 0.2003, 0.1996, 0.1995],\n",
      "        [0.2000, 0.1995, 0.2005, 0.2004, 0.1996],\n",
      "        [0.2000, 0.2006, 0.1997, 0.2000, 0.1996],\n",
      "        [0.2007, 0.2000, 0.2006, 0.1990, 0.1998],\n",
      "        [0.2004, 0.1998, 0.1993, 0.1996, 0.2009],\n",
      "        [0.1986, 0.1995, 0.2004, 0.2008, 0.2007],\n",
      "        [0.2000, 0.2001, 0.1999, 0.1998, 0.2002],\n",
      "        [0.1996, 0.1998, 0.2000, 0.1998, 0.2008],\n",
      "        [0.2001, 0.1995, 0.2003, 0.2007, 0.1995],\n",
      "        [0.1996, 0.2000, 0.2000, 0.2000, 0.2003],\n",
      "        [0.2001, 0.2001, 0.2002, 0.1999, 0.1997],\n",
      "        [0.1998, 0.1995, 0.1998, 0.2006, 0.2002],\n",
      "        [0.2000, 0.2002, 0.1997, 0.1995, 0.2006],\n",
      "        [0.1999, 0.1998, 0.1996, 0.1996, 0.2011],\n",
      "        [0.1996, 0.2003, 0.2003, 0.2002, 0.1997],\n",
      "        [0.2007, 0.2002, 0.1995, 0.1992, 0.2004],\n",
      "        [0.2002, 0.2003, 0.2000, 0.2004, 0.1991],\n",
      "        [0.1998, 0.2001, 0.2009, 0.1991, 0.2001]], grad_fn=<SoftmaxBackward0>)\n",
      "Input shape for user encoder: torch.Size([32, 10, 30])\n",
      "Input shape for news encoder: torch.Size([32, 5, 30])\n",
      "embedded sequence size:  torch.Size([320, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([320, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([320, 768])\n",
      "Click title presents:  torch.Size([32, 10, 768])\n",
      "Multihead attention output from user encoder:  torch.Size([32, 10, 768])\n",
      "User encoder output after additive:  torch.Size([32, 768])\n",
      "embedded sequence size:  torch.Size([160, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([160, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([160, 768])\n",
      "User present shape:  torch.Size([32, 1, 768])\n",
      "News present shape:  torch.Size([32, 5, 768])\n",
      "Preds after fot shape:  torch.Size([32, 5])\n",
      "Preds shape:  torch.Size([32, 5])\n",
      "tensor([[0.1988, 0.1955, 0.2063, 0.1958, 0.2036],\n",
      "        [0.1949, 0.1986, 0.1938, 0.2119, 0.2008],\n",
      "        [0.1987, 0.1932, 0.1995, 0.2047, 0.2039],\n",
      "        [0.1991, 0.1972, 0.2011, 0.1968, 0.2057],\n",
      "        [0.2035, 0.2022, 0.1961, 0.1990, 0.1991],\n",
      "        [0.1915, 0.2024, 0.1982, 0.1966, 0.2113],\n",
      "        [0.2104, 0.1932, 0.1977, 0.1935, 0.2052],\n",
      "        [0.2014, 0.1972, 0.2058, 0.1916, 0.2041],\n",
      "        [0.2032, 0.1919, 0.2005, 0.2021, 0.2023],\n",
      "        [0.1921, 0.2041, 0.2050, 0.1975, 0.2013],\n",
      "        [0.1999, 0.1972, 0.2076, 0.1942, 0.2011],\n",
      "        [0.2008, 0.2056, 0.1939, 0.2038, 0.1959],\n",
      "        [0.2005, 0.1995, 0.1971, 0.1992, 0.2036],\n",
      "        [0.2082, 0.1931, 0.2052, 0.1983, 0.1952],\n",
      "        [0.1993, 0.1983, 0.1973, 0.2066, 0.1985],\n",
      "        [0.1999, 0.2078, 0.1930, 0.1970, 0.2022],\n",
      "        [0.2009, 0.1967, 0.2023, 0.2002, 0.1998],\n",
      "        [0.1883, 0.2075, 0.2057, 0.2035, 0.1950],\n",
      "        [0.1971, 0.2018, 0.1964, 0.1977, 0.2070],\n",
      "        [0.2065, 0.2014, 0.2015, 0.1911, 0.1995],\n",
      "        [0.2029, 0.2067, 0.1907, 0.1990, 0.2007],\n",
      "        [0.1986, 0.2087, 0.2004, 0.1931, 0.1992],\n",
      "        [0.1937, 0.2092, 0.1965, 0.2033, 0.1973],\n",
      "        [0.1918, 0.2069, 0.2016, 0.2062, 0.1935],\n",
      "        [0.1959, 0.1965, 0.2024, 0.2064, 0.1988],\n",
      "        [0.1997, 0.2025, 0.2052, 0.1910, 0.2016],\n",
      "        [0.2026, 0.1976, 0.2017, 0.1963, 0.2018],\n",
      "        [0.1980, 0.1989, 0.1945, 0.2016, 0.2070],\n",
      "        [0.2073, 0.1961, 0.1989, 0.1951, 0.2027],\n",
      "        [0.1988, 0.1996, 0.1945, 0.2033, 0.2038],\n",
      "        [0.1900, 0.2010, 0.2062, 0.1948, 0.2079],\n",
      "        [0.2053, 0.1922, 0.1972, 0.2110, 0.1943]], grad_fn=<SoftmaxBackward0>)\n",
      "Input shape for user encoder: torch.Size([32, 10, 30])\n",
      "Input shape for news encoder: torch.Size([32, 5, 30])\n",
      "embedded sequence size:  torch.Size([320, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([320, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([320, 768])\n",
      "Click title presents:  torch.Size([32, 10, 768])\n",
      "Multihead attention output from user encoder:  torch.Size([32, 10, 768])\n",
      "User encoder output after additive:  torch.Size([32, 768])\n",
      "embedded sequence size:  torch.Size([160, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([160, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([160, 768])\n",
      "User present shape:  torch.Size([32, 1, 768])\n",
      "News present shape:  torch.Size([32, 5, 768])\n",
      "Preds after fot shape:  torch.Size([32, 5])\n",
      "Preds shape:  torch.Size([32, 5])\n",
      "tensor([[0.2243, 0.2019, 0.1774, 0.2126, 0.1838],\n",
      "        [0.1615, 0.2015, 0.2255, 0.2319, 0.1796],\n",
      "        [0.1848, 0.2019, 0.2065, 0.1964, 0.2104],\n",
      "        [0.1890, 0.2058, 0.1956, 0.2026, 0.2070],\n",
      "        [0.1964, 0.2322, 0.1964, 0.1719, 0.2031],\n",
      "        [0.1902, 0.1810, 0.2173, 0.2226, 0.1890],\n",
      "        [0.1858, 0.2324, 0.2370, 0.1641, 0.1807],\n",
      "        [0.1705, 0.2197, 0.1761, 0.2260, 0.2077],\n",
      "        [0.1961, 0.2149, 0.2088, 0.2033, 0.1768],\n",
      "        [0.1768, 0.1714, 0.1957, 0.2438, 0.2122],\n",
      "        [0.1764, 0.2517, 0.2013, 0.2049, 0.1658],\n",
      "        [0.1920, 0.1660, 0.1865, 0.2081, 0.2474],\n",
      "        [0.2295, 0.1743, 0.2054, 0.1958, 0.1949],\n",
      "        [0.1923, 0.2278, 0.1819, 0.2005, 0.1975],\n",
      "        [0.2134, 0.1717, 0.2147, 0.1909, 0.2093],\n",
      "        [0.2224, 0.1944, 0.2017, 0.1897, 0.1917],\n",
      "        [0.1855, 0.1896, 0.1914, 0.2141, 0.2195],\n",
      "        [0.2021, 0.1933, 0.2310, 0.1696, 0.2040],\n",
      "        [0.2167, 0.1937, 0.1879, 0.1964, 0.2054],\n",
      "        [0.1663, 0.2048, 0.1976, 0.1893, 0.2420],\n",
      "        [0.1935, 0.2076, 0.2210, 0.1427, 0.2353],\n",
      "        [0.2063, 0.1822, 0.2089, 0.2245, 0.1782],\n",
      "        [0.1660, 0.1954, 0.2084, 0.2429, 0.1872],\n",
      "        [0.1906, 0.1913, 0.1954, 0.2407, 0.1819],\n",
      "        [0.1960, 0.2170, 0.2133, 0.1494, 0.2243],\n",
      "        [0.1881, 0.2374, 0.2112, 0.2255, 0.1378],\n",
      "        [0.1872, 0.2355, 0.1813, 0.1930, 0.2031],\n",
      "        [0.1800, 0.2345, 0.1889, 0.2095, 0.1870],\n",
      "        [0.2205, 0.2097, 0.1932, 0.2142, 0.1624],\n",
      "        [0.1822, 0.1985, 0.2078, 0.1899, 0.2216],\n",
      "        [0.2135, 0.1816, 0.2136, 0.1534, 0.2379],\n",
      "        [0.2430, 0.2200, 0.1618, 0.2142, 0.1610]], grad_fn=<SoftmaxBackward0>)\n",
      "Input shape for user encoder: torch.Size([32, 10, 30])\n",
      "Input shape for news encoder: torch.Size([32, 5, 30])\n",
      "embedded sequence size:  torch.Size([320, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([320, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([320, 768])\n",
      "Click title presents:  torch.Size([32, 10, 768])\n",
      "Multihead attention output from user encoder:  torch.Size([32, 10, 768])\n",
      "User encoder output after additive:  torch.Size([32, 768])\n",
      "embedded sequence size:  torch.Size([160, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([160, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([160, 768])\n",
      "User present shape:  torch.Size([32, 1, 768])\n",
      "News present shape:  torch.Size([32, 5, 768])\n",
      "Preds after fot shape:  torch.Size([32, 5])\n",
      "Preds shape:  torch.Size([32, 5])\n",
      "tensor([[0.2089, 0.2030, 0.1760, 0.2000, 0.2121],\n",
      "        [0.2190, 0.2491, 0.1825, 0.1734, 0.1760],\n",
      "        [0.2041, 0.2598, 0.2004, 0.1366, 0.1990],\n",
      "        [0.2174, 0.2421, 0.2041, 0.1719, 0.1645],\n",
      "        [0.2370, 0.1952, 0.2088, 0.1502, 0.2088],\n",
      "        [0.1767, 0.2285, 0.1692, 0.1733, 0.2524],\n",
      "        [0.1857, 0.2008, 0.2000, 0.1522, 0.2612],\n",
      "        [0.2045, 0.2771, 0.1535, 0.1940, 0.1709],\n",
      "        [0.2043, 0.2146, 0.2074, 0.1828, 0.1909],\n",
      "        [0.2023, 0.1731, 0.2342, 0.1802, 0.2102],\n",
      "        [0.1761, 0.2452, 0.1619, 0.2576, 0.1592],\n",
      "        [0.2010, 0.1755, 0.2399, 0.1711, 0.2124],\n",
      "        [0.2061, 0.2260, 0.1805, 0.1817, 0.2057],\n",
      "        [0.2043, 0.2187, 0.1699, 0.1886, 0.2185],\n",
      "        [0.1688, 0.2594, 0.1852, 0.2150, 0.1716],\n",
      "        [0.1931, 0.2185, 0.1690, 0.1742, 0.2452],\n",
      "        [0.2040, 0.2142, 0.1697, 0.2399, 0.1723],\n",
      "        [0.2180, 0.1690, 0.2131, 0.1808, 0.2192],\n",
      "        [0.1839, 0.2052, 0.2077, 0.2153, 0.1878],\n",
      "        [0.2142, 0.2041, 0.2296, 0.2097, 0.1424],\n",
      "        [0.1895, 0.2216, 0.1584, 0.1833, 0.2471],\n",
      "        [0.2221, 0.1828, 0.1988, 0.1791, 0.2172],\n",
      "        [0.1826, 0.2164, 0.2050, 0.1972, 0.1987],\n",
      "        [0.2188, 0.1877, 0.1852, 0.2251, 0.1832],\n",
      "        [0.2292, 0.2200, 0.1416, 0.2202, 0.1889],\n",
      "        [0.1973, 0.1757, 0.2168, 0.2225, 0.1877],\n",
      "        [0.2245, 0.1941, 0.1597, 0.2259, 0.1958],\n",
      "        [0.2239, 0.1841, 0.1636, 0.2207, 0.2078],\n",
      "        [0.2286, 0.1863, 0.2270, 0.1738, 0.1842],\n",
      "        [0.1718, 0.1792, 0.1982, 0.1732, 0.2776],\n",
      "        [0.1958, 0.1709, 0.1964, 0.2055, 0.2315],\n",
      "        [0.2415, 0.1760, 0.2142, 0.1826, 0.1857]], grad_fn=<SoftmaxBackward0>)\n",
      "Input shape for user encoder: torch.Size([32, 10, 30])\n",
      "Input shape for news encoder: torch.Size([32, 5, 30])\n",
      "embedded sequence size:  torch.Size([320, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([320, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([320, 768])\n",
      "Click title presents:  torch.Size([32, 10, 768])\n",
      "Multihead attention output from user encoder:  torch.Size([32, 10, 768])\n",
      "User encoder output after additive:  torch.Size([32, 768])\n",
      "embedded sequence size:  torch.Size([160, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([160, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([160, 768])\n",
      "User present shape:  torch.Size([32, 1, 768])\n",
      "News present shape:  torch.Size([32, 5, 768])\n",
      "Preds after fot shape:  torch.Size([32, 5])\n",
      "Preds shape:  torch.Size([32, 5])\n",
      "tensor([[0.1691, 0.1542, 0.2027, 0.2217, 0.2524],\n",
      "        [0.1770, 0.2337, 0.1859, 0.2019, 0.2015],\n",
      "        [0.2325, 0.2232, 0.2178, 0.1512, 0.1752],\n",
      "        [0.2599, 0.1814, 0.1433, 0.2605, 0.1549],\n",
      "        [0.1683, 0.1976, 0.1571, 0.2268, 0.2502],\n",
      "        [0.2336, 0.1800, 0.1734, 0.2225, 0.1904],\n",
      "        [0.1528, 0.2345, 0.2371, 0.1611, 0.2145],\n",
      "        [0.2313, 0.1658, 0.1808, 0.2590, 0.1632],\n",
      "        [0.2663, 0.2225, 0.1651, 0.2019, 0.1442],\n",
      "        [0.2042, 0.1692, 0.2429, 0.1751, 0.2087],\n",
      "        [0.2459, 0.2287, 0.2197, 0.1746, 0.1310],\n",
      "        [0.1778, 0.1772, 0.2332, 0.1975, 0.2142],\n",
      "        [0.1997, 0.2648, 0.1979, 0.1610, 0.1766],\n",
      "        [0.1609, 0.2160, 0.2118, 0.1836, 0.2277],\n",
      "        [0.2573, 0.1848, 0.1135, 0.2067, 0.2378],\n",
      "        [0.2297, 0.2423, 0.2050, 0.1253, 0.1978],\n",
      "        [0.1394, 0.1908, 0.2446, 0.2139, 0.2113],\n",
      "        [0.2354, 0.1997, 0.1404, 0.1983, 0.2263],\n",
      "        [0.1683, 0.2252, 0.2027, 0.1876, 0.2162],\n",
      "        [0.1421, 0.2636, 0.2406, 0.2008, 0.1529],\n",
      "        [0.2401, 0.2036, 0.1737, 0.1830, 0.1997],\n",
      "        [0.1743, 0.2028, 0.1890, 0.2308, 0.2031],\n",
      "        [0.1573, 0.1782, 0.2026, 0.1960, 0.2658],\n",
      "        [0.2107, 0.2287, 0.2012, 0.2039, 0.1555],\n",
      "        [0.2294, 0.1678, 0.2282, 0.2356, 0.1391],\n",
      "        [0.2085, 0.2472, 0.2014, 0.2076, 0.1354],\n",
      "        [0.2126, 0.2123, 0.2065, 0.1934, 0.1753],\n",
      "        [0.2352, 0.2505, 0.1959, 0.1868, 0.1316],\n",
      "        [0.1728, 0.2120, 0.1472, 0.1901, 0.2780],\n",
      "        [0.1971, 0.2037, 0.1553, 0.1733, 0.2705],\n",
      "        [0.1570, 0.1769, 0.1757, 0.2837, 0.2067],\n",
      "        [0.3042, 0.1886, 0.1701, 0.1692, 0.1678]], grad_fn=<SoftmaxBackward0>)\n",
      "Input shape for user encoder: torch.Size([32, 10, 30])\n",
      "Input shape for news encoder: torch.Size([32, 5, 30])\n",
      "embedded sequence size:  torch.Size([320, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([320, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([320, 768])\n",
      "Click title presents:  torch.Size([32, 10, 768])\n",
      "Multihead attention output from user encoder:  torch.Size([32, 10, 768])\n",
      "User encoder output after additive:  torch.Size([32, 768])\n",
      "embedded sequence size:  torch.Size([160, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([160, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([160, 768])\n",
      "User present shape:  torch.Size([32, 1, 768])\n",
      "News present shape:  torch.Size([32, 5, 768])\n",
      "Preds after fot shape:  torch.Size([32, 5])\n",
      "Preds shape:  torch.Size([32, 5])\n",
      "tensor([[0.2082, 0.2033, 0.1987, 0.2408, 0.1490],\n",
      "        [0.1498, 0.2409, 0.1710, 0.2741, 0.1643],\n",
      "        [0.1885, 0.2972, 0.2065, 0.1115, 0.1963],\n",
      "        [0.1291, 0.2408, 0.2483, 0.1759, 0.2059],\n",
      "        [0.2593, 0.1434, 0.2303, 0.1766, 0.1904],\n",
      "        [0.2442, 0.1867, 0.2271, 0.1867, 0.1553],\n",
      "        [0.2642, 0.1892, 0.1630, 0.1688, 0.2148],\n",
      "        [0.2520, 0.1706, 0.1531, 0.2913, 0.1329],\n",
      "        [0.2673, 0.1712, 0.1057, 0.1954, 0.2604],\n",
      "        [0.2649, 0.2262, 0.1602, 0.1614, 0.1872],\n",
      "        [0.1452, 0.1492, 0.1581, 0.2034, 0.3440],\n",
      "        [0.1397, 0.1716, 0.1800, 0.2944, 0.2144],\n",
      "        [0.2066, 0.2071, 0.1623, 0.2434, 0.1805],\n",
      "        [0.1695, 0.1174, 0.2976, 0.2702, 0.1453],\n",
      "        [0.1255, 0.2044, 0.1546, 0.4157, 0.0997],\n",
      "        [0.1746, 0.2497, 0.2010, 0.1288, 0.2459],\n",
      "        [0.1355, 0.2209, 0.2158, 0.1581, 0.2698],\n",
      "        [0.1879, 0.2444, 0.2927, 0.1522, 0.1229],\n",
      "        [0.2265, 0.1009, 0.2174, 0.1680, 0.2872],\n",
      "        [0.1399, 0.2957, 0.3193, 0.1261, 0.1189],\n",
      "        [0.1266, 0.2142, 0.2544, 0.1852, 0.2196],\n",
      "        [0.2240, 0.1787, 0.1754, 0.1678, 0.2541],\n",
      "        [0.2437, 0.1678, 0.1939, 0.2340, 0.1607],\n",
      "        [0.1947, 0.2002, 0.1318, 0.2700, 0.2033],\n",
      "        [0.2351, 0.1847, 0.1786, 0.2100, 0.1915],\n",
      "        [0.2266, 0.2188, 0.1867, 0.1866, 0.1813],\n",
      "        [0.1241, 0.2701, 0.2304, 0.1457, 0.2297],\n",
      "        [0.1888, 0.1977, 0.1712, 0.2357, 0.2067],\n",
      "        [0.1894, 0.1861, 0.2666, 0.1416, 0.2162],\n",
      "        [0.2548, 0.1607, 0.2634, 0.1343, 0.1868],\n",
      "        [0.1287, 0.1387, 0.1882, 0.2519, 0.2925],\n",
      "        [0.1582, 0.2223, 0.2358, 0.1894, 0.1943]], grad_fn=<SoftmaxBackward0>)\n",
      "Input shape for user encoder: torch.Size([32, 10, 30])\n",
      "Input shape for news encoder: torch.Size([32, 5, 30])\n",
      "embedded sequence size:  torch.Size([320, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([320, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([320, 768])\n",
      "Click title presents:  torch.Size([32, 10, 768])\n",
      "Multihead attention output from user encoder:  torch.Size([32, 10, 768])\n",
      "User encoder output after additive:  torch.Size([32, 768])\n",
      "embedded sequence size:  torch.Size([160, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([160, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([160, 768])\n",
      "User present shape:  torch.Size([32, 1, 768])\n",
      "News present shape:  torch.Size([32, 5, 768])\n",
      "Preds after fot shape:  torch.Size([32, 5])\n",
      "Preds shape:  torch.Size([32, 5])\n",
      "tensor([[0.0529, 0.1658, 0.2251, 0.2543, 0.3020],\n",
      "        [0.2046, 0.0854, 0.1482, 0.4826, 0.0791],\n",
      "        [0.2835, 0.1391, 0.2089, 0.1649, 0.2036],\n",
      "        [0.1405, 0.0892, 0.3102, 0.2293, 0.2307],\n",
      "        [0.1475, 0.2832, 0.1570, 0.1503, 0.2621],\n",
      "        [0.1158, 0.1609, 0.2502, 0.0856, 0.3876],\n",
      "        [0.4186, 0.0468, 0.0921, 0.2993, 0.1432],\n",
      "        [0.3696, 0.1479, 0.0995, 0.1750, 0.2079],\n",
      "        [0.2702, 0.0709, 0.2576, 0.2974, 0.1039],\n",
      "        [0.1925, 0.1970, 0.0866, 0.1647, 0.3592],\n",
      "        [0.0802, 0.1910, 0.0803, 0.3160, 0.3326],\n",
      "        [0.2782, 0.0794, 0.2801, 0.1719, 0.1904],\n",
      "        [0.1111, 0.2732, 0.2126, 0.2689, 0.1342],\n",
      "        [0.2689, 0.3119, 0.0540, 0.2140, 0.1512],\n",
      "        [0.1349, 0.5526, 0.0433, 0.0877, 0.1815],\n",
      "        [0.3837, 0.1590, 0.1299, 0.1669, 0.1606],\n",
      "        [0.1696, 0.2568, 0.3090, 0.0709, 0.1938],\n",
      "        [0.1325, 0.1614, 0.2080, 0.3084, 0.1897],\n",
      "        [0.3074, 0.1592, 0.1280, 0.2158, 0.1897],\n",
      "        [0.0690, 0.2508, 0.2153, 0.2607, 0.2042],\n",
      "        [0.1106, 0.2853, 0.1777, 0.2471, 0.1794],\n",
      "        [0.0957, 0.1901, 0.1189, 0.1706, 0.4247],\n",
      "        [0.2348, 0.3030, 0.2220, 0.0850, 0.1552],\n",
      "        [0.1788, 0.1570, 0.3305, 0.1537, 0.1799],\n",
      "        [0.0897, 0.2813, 0.4198, 0.0820, 0.1273],\n",
      "        [0.1035, 0.1447, 0.3289, 0.2800, 0.1429],\n",
      "        [0.2226, 0.0789, 0.2824, 0.1896, 0.2265],\n",
      "        [0.1217, 0.2407, 0.2287, 0.2943, 0.1146],\n",
      "        [0.1514, 0.1237, 0.1027, 0.1923, 0.4299],\n",
      "        [0.5002, 0.1026, 0.1354, 0.1859, 0.0759],\n",
      "        [0.4688, 0.1308, 0.1332, 0.1072, 0.1600],\n",
      "        [0.1303, 0.3442, 0.1503, 0.1040, 0.2711]], grad_fn=<SoftmaxBackward0>)\n",
      "Input shape for user encoder: torch.Size([24, 10, 30])\n",
      "Input shape for news encoder: torch.Size([24, 5, 30])\n",
      "embedded sequence size:  torch.Size([240, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([240, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([240, 768])\n",
      "Click title presents:  torch.Size([24, 10, 768])\n",
      "Multihead attention output from user encoder:  torch.Size([24, 10, 768])\n",
      "User encoder output after additive:  torch.Size([24, 768])\n",
      "embedded sequence size:  torch.Size([120, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([120, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([120, 768])\n",
      "User present shape:  torch.Size([24, 1, 768])\n",
      "News present shape:  torch.Size([24, 5, 768])\n",
      "Preds after fot shape:  torch.Size([24, 5])\n",
      "Preds shape:  torch.Size([24, 5])\n",
      "tensor([[0.4223, 0.1571, 0.0720, 0.3381, 0.0105],\n",
      "        [0.1282, 0.0240, 0.4430, 0.0162, 0.3887],\n",
      "        [0.0676, 0.0748, 0.1930, 0.1256, 0.5390],\n",
      "        [0.0514, 0.1395, 0.1593, 0.1598, 0.4900],\n",
      "        [0.1464, 0.0712, 0.2191, 0.2254, 0.3378],\n",
      "        [0.2059, 0.2967, 0.3171, 0.0976, 0.0827],\n",
      "        [0.1599, 0.0531, 0.2614, 0.1371, 0.3885],\n",
      "        [0.5196, 0.1478, 0.0793, 0.0160, 0.2374],\n",
      "        [0.0408, 0.4789, 0.0968, 0.1727, 0.2109],\n",
      "        [0.0429, 0.2915, 0.1676, 0.1941, 0.3038],\n",
      "        [0.1037, 0.0761, 0.3314, 0.4116, 0.0772],\n",
      "        [0.0128, 0.2641, 0.0948, 0.5457, 0.0827],\n",
      "        [0.0995, 0.0951, 0.0160, 0.2435, 0.5459],\n",
      "        [0.1705, 0.3096, 0.1451, 0.2347, 0.1401],\n",
      "        [0.3622, 0.2093, 0.1849, 0.1389, 0.1046],\n",
      "        [0.1109, 0.3851, 0.2730, 0.1138, 0.1172],\n",
      "        [0.2827, 0.1131, 0.4616, 0.0923, 0.0503],\n",
      "        [0.2609, 0.1389, 0.1974, 0.3157, 0.0871],\n",
      "        [0.4405, 0.2391, 0.0319, 0.0894, 0.1991],\n",
      "        [0.3650, 0.1806, 0.2749, 0.0563, 0.1233],\n",
      "        [0.3517, 0.0780, 0.0630, 0.0593, 0.4480],\n",
      "        [0.3525, 0.2455, 0.1661, 0.1923, 0.0437],\n",
      "        [0.3774, 0.3466, 0.1095, 0.0402, 0.1262],\n",
      "        [0.1009, 0.5956, 0.0347, 0.1645, 0.1043]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_14804\\4166849013.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  his_input_title = torch.tensor(his_input_title, dtype=torch.long).to(device)\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_14804\\4166849013.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred_input_title = torch.tensor(pred_input_title, dtype=torch.long).to(device)\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_14804\\4166849013.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded sequence size:  torch.Size([1870, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([1870, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([1870, 768])\n",
      "Click title presents:  torch.Size([187, 10, 768])\n",
      "Multihead attention output from user encoder:  torch.Size([187, 10, 768])\n",
      "User encoder output after additive:  torch.Size([187, 768])\n",
      "embedded sequence size:  torch.Size([187, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([187, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([187, 768])\n",
      "User present shape:  torch.Size([187, 1, 768])\n",
      "News present shape:  torch.Size([187, 1, 768])\n",
      "Preds after fot shape:  torch.Size([187, 1])\n",
      "Preds shape:  torch.Size([187, 1])\n",
      "embedded sequence size:  torch.Size([2290, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([2290, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([2290, 768])\n",
      "Click title presents:  torch.Size([229, 10, 768])\n",
      "Multihead attention output from user encoder:  torch.Size([229, 10, 768])\n",
      "User encoder output after additive:  torch.Size([229, 768])\n",
      "embedded sequence size:  torch.Size([229, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([229, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([229, 768])\n",
      "User present shape:  torch.Size([229, 1, 768])\n",
      "News present shape:  torch.Size([229, 1, 768])\n",
      "Preds after fot shape:  torch.Size([229, 1])\n",
      "Preds shape:  torch.Size([229, 1])\n",
      "embedded sequence size:  torch.Size([1840, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([1840, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([1840, 768])\n",
      "Click title presents:  torch.Size([184, 10, 768])\n",
      "Multihead attention output from user encoder:  torch.Size([184, 10, 768])\n",
      "User encoder output after additive:  torch.Size([184, 768])\n",
      "embedded sequence size:  torch.Size([184, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([184, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([184, 768])\n",
      "User present shape:  torch.Size([184, 1, 768])\n",
      "News present shape:  torch.Size([184, 1, 768])\n",
      "Preds after fot shape:  torch.Size([184, 1])\n",
      "Preds shape:  torch.Size([184, 1])\n",
      "embedded sequence size:  torch.Size([2540, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([2540, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([2540, 768])\n",
      "Click title presents:  torch.Size([254, 10, 768])\n",
      "Multihead attention output from user encoder:  torch.Size([254, 10, 768])\n",
      "User encoder output after additive:  torch.Size([254, 768])\n",
      "embedded sequence size:  torch.Size([254, 30, 768])\n",
      "News encoder multihead attention output:  torch.Size([254, 30, 768])\n",
      "pred title after attention in news encoder:  torch.Size([254, 768])\n",
      "User present shape:  torch.Size([254, 1, 768])\n",
      "News present shape:  torch.Size([254, 1, 768])\n",
      "Preds after fot shape:  torch.Size([254, 1])\n",
      "Preds shape:  torch.Size([254, 1])\n",
      "embedded sequence size:  torch.Size([2950, 30, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 25063200000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     54\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnrms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhis_input_title\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_input_title\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     58\u001b[0m val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Danmarks Tekniske Universitet\\Autumn-2024\\news-recommendation\\src\\ebrec\\models\\newsrec_pytorch\\NRMSModel.py:19\u001b[0m, in \u001b[0;36mNRMSModel.forward\u001b[1;34m(self, his_input_title, pred_input_title)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, his_input_title, pred_input_title):\n\u001b[0;32m     17\u001b[0m     batch_size, npratio, title_size \u001b[38;5;241m=\u001b[39m pred_input_title\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 19\u001b[0m     user_present \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhis_input_title\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     user_present_unsqueezed \u001b[38;5;241m=\u001b[39m user_present\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m     pred_input_title_flat \u001b[38;5;241m=\u001b[39m pred_input_title\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, title_size)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Danmarks Tekniske Universitet\\Autumn-2024\\news-recommendation\\src\\ebrec\\models\\newsrec_pytorch\\user_encoder.py:19\u001b[0m, in \u001b[0;36mUserEncoder.forward\u001b[1;34m(self, his_input_title)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Reshape for titleencoder: treat each news title independently\u001b[39;00m\n\u001b[0;32m     18\u001b[0m his_input_title_flat \u001b[38;5;241m=\u001b[39m his_input_title\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, title_size)  \u001b[38;5;66;03m# Shape: (batch_size * history_size, title_size)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m click_title_presents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnews_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhis_input_title_flat\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (batch_size * history_size, hidden_dim)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Reshape back to include history_size\u001b[39;00m\n\u001b[0;32m     22\u001b[0m click_title_presents \u001b[38;5;241m=\u001b[39m click_title_presents\u001b[38;5;241m.\u001b[39mview(batch_size, history_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (batch_size, history_size, hidden_dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Danmarks Tekniske Universitet\\Autumn-2024\\news-recommendation\\src\\ebrec\\models\\newsrec_pytorch\\news_encoder.py:19\u001b[0m, in \u001b[0;36mNewsEncoder.forward\u001b[1;34m(self, sequences_input_title)\u001b[0m\n\u001b[0;32m     17\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embedded_sequences)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# print(y.shape)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNews encoder multihead attention output: \u001b[39m\u001b[38;5;124m\"\u001b[39m, y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# print(y.shape)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\functional.py:5442\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5441\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m-> 5442\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m   5444\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m dropout(attn_output_weights, p\u001b[38;5;241m=\u001b[39mdropout_p)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1856\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1860\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 25063200000 bytes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "epoch = 0\n",
    "num_epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "from ebrec.models.newsrec_pytorch import NRMSModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "word2vec_embedding = torch.tensor(word2vec_embedding, dtype=torch.float32).to(device)\n",
    "print(word2vec_embedding.shape)\n",
    "nrms = NRMSModel(hparams_nrms=hparams_nrms, word2vec_embedding=word2vec_embedding, seed=42)\n",
    "print(nrms)\n",
    "optimizer = torch.optim.Adam(nrms.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "writer = SummaryWriter(\"./logs\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    nrms.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for i, ((his_input_title, pred_input_title), labels) in enumerate(train_dataloader):\n",
    "        his_input_title = torch.tensor(his_input_title, dtype=torch.long).to(device)\n",
    "        pred_input_title = torch.tensor(pred_input_title, dtype=torch.long).to(device)\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "        labels = labels.view(-1)\n",
    "        print(\"Input shape for user encoder:\", his_input_title.shape)\n",
    "        print(\"Input shape for news encoder:\", pred_input_title.shape)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = nrms(his_input_title, pred_input_title)  # Forward pass\n",
    "        print(outputs)\n",
    "        loss = loss_fn(outputs.view(-1), labels.float())  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the parameters\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    # Validation\n",
    "    nrms.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (his_input_title, pred_input_title), labels in val_dataloader:\n",
    "            his_input_title = torch.tensor(his_input_title, dtype=torch.long).to(device)\n",
    "            pred_input_title = torch.tensor(pred_input_title, dtype=torch.long).to(device)\n",
    "            \n",
    "            labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            outputs = nrms(his_input_title, pred_input_title)\n",
    "            loss = loss_fn(outputs.view(-1), labels.float())\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    val_loss /= len(val_dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {val_loss:.3f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "       # Log to TensorBoard\n",
    "    writer.add_scalar(\"Loss/train\", running_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/validation\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/validation\", accuracy, epoch)\n",
    "\n",
    "# Save the model weights\n",
    "MODEL_NAME = \"NRMS\"\n",
    "MODEL_WEIGHTS = f\"downloads/data/state_dict/{MODEL_NAME}/weights\"\n",
    "torch.save(nrms.state_dict(), MODEL_WEIGHTS)\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 597s 8s/step - loss: 1.5952 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "MODEL_WEIGHTS = f\"downloads/data/state_dict/{MODEL_NAME}/weights\"\n",
    "\n",
    "# CALLBACKS\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "# modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=MODEL_WEIGHTS, save_best_only=True, save_weights_only=True, verbose=1\n",
    "# )\n",
    "\n",
    "hparams_nrms.history_size = HISTORY_SIZE\n",
    "\n",
    "\n",
    "model = NRMSModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "hist = model.model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=1,\n",
    "    # callbacks=[tensorboard_callback, early_stopping, modelcheckpoint],\n",
    ")\n",
    "# Uncomment the following line if you have pre-trained weights\n",
    "# _ = model.model.load_weights(filepath=MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example how to compute some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 383s 2s/step\n"
     ]
    }
   ],
   "source": [
    "pred_validation = model.scorer.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the predictions to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>scores</th><th>is_known_user</th><th>ranked_scores</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>bool</td><td>list[i64]</td></tr></thead><tbody><tr><td>386380</td><td>[9779867, 9779423, … 9779956]</td><td>[9787931, 9780460, … 9765777]</td><td>[9787465]</td><td>43657242</td><td>[0, 0, … 0]</td><td>[0.456477, 0.528305, … 0.508259]</td><td>true</td><td>[35, 13, … 23]</td></tr><tr><td>177925</td><td>[9777910, 9777769, … 9779511]</td><td>[8560195, 9486080, … 9790091]</td><td>[9789922]</td><td>110129832</td><td>[0, 0, … 0]</td><td>[0.520847, 0.433627, … 0.570924]</td><td>false</td><td>[3, 6, … 1]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 9)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ article_i ┆ article_i ┆ … ┆ labels    ┆ scores    ┆ is_known_ ┆ ranked_sc │\n",
       "│ ---     ┆ _fixed     ┆ ds_inview ┆ ds_clicke ┆   ┆ ---       ┆ ---       ┆ user      ┆ ores      │\n",
       "│ u32     ┆ ---        ┆ ---       ┆ d         ┆   ┆ list[i8]  ┆ list[f64] ┆ ---       ┆ ---       │\n",
       "│         ┆ list[i32]  ┆ list[i32] ┆ ---       ┆   ┆           ┆           ┆ bool      ┆ list[i64] │\n",
       "│         ┆            ┆           ┆ list[i32] ┆   ┆           ┆           ┆           ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 386380  ┆ [9779867,  ┆ [9787931, ┆ [9787465] ┆ … ┆ [0, 0, …  ┆ [0.456477 ┆ true      ┆ [35, 13,  │\n",
       "│         ┆ 9779423, … ┆ 9780460,  ┆           ┆   ┆ 0]        ┆ ,         ┆           ┆ … 23]     │\n",
       "│         ┆ 9779956]   ┆ …         ┆           ┆   ┆           ┆ 0.528305, ┆           ┆           │\n",
       "│         ┆            ┆ 9765777]  ┆           ┆   ┆           ┆ …         ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 0.508259] ┆           ┆           │\n",
       "│ 177925  ┆ [9777910,  ┆ [8560195, ┆ [9789922] ┆ … ┆ [0, 0, …  ┆ [0.520847 ┆ false     ┆ [3, 6, …  │\n",
       "│         ┆ 9777769, … ┆ 9486080,  ┆           ┆   ┆ 0]        ┆ ,         ┆           ┆ 1]        │\n",
       "│         ┆ 9779511]   ┆ …         ┆           ┆   ┆           ┆ 0.433627, ┆           ┆           │\n",
       "│         ┆            ┆ 9790091]  ┆           ┆   ┆           ┆ …         ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 0.570924] ┆           ┆           │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "    add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    ")\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.523276170069743,\n",
       "    \"mrr\": 0.3271357380208631,\n",
       "    \"ndcg@5\": 0.368758819449155,\n",
       "    \"ndcg@10\": 0.44821698782392533\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>scores</th><th>is_known_user</th><th>ranked_scores</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>bool</td><td>list[i64]</td></tr></thead><tbody><tr><td>386380</td><td>[9779867, 9779423, … 9779956]</td><td>[9787931, 9780460, … 9765777]</td><td>[9787465]</td><td>43657242</td><td>[0, 0, … 0]</td><td>[0.456477, 0.528305, … 0.508259]</td><td>true</td><td>[35, 13, … 23]</td></tr><tr><td>177925</td><td>[9777910, 9777769, … 9779511]</td><td>[8560195, 9486080, … 9790091]</td><td>[9789922]</td><td>110129832</td><td>[0, 0, … 0]</td><td>[0.520847, 0.433627, … 0.570924]</td><td>false</td><td>[3, 6, … 1]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 9)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ article_i ┆ article_i ┆ … ┆ labels    ┆ scores    ┆ is_known_ ┆ ranked_sc │\n",
       "│ ---     ┆ _fixed     ┆ ds_inview ┆ ds_clicke ┆   ┆ ---       ┆ ---       ┆ user      ┆ ores      │\n",
       "│ u32     ┆ ---        ┆ ---       ┆ d         ┆   ┆ list[i8]  ┆ list[f64] ┆ ---       ┆ ---       │\n",
       "│         ┆ list[i32]  ┆ list[i32] ┆ ---       ┆   ┆           ┆           ┆ bool      ┆ list[i64] │\n",
       "│         ┆            ┆           ┆ list[i32] ┆   ┆           ┆           ┆           ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 386380  ┆ [9779867,  ┆ [9787931, ┆ [9787465] ┆ … ┆ [0, 0, …  ┆ [0.456477 ┆ true      ┆ [35, 13,  │\n",
       "│         ┆ 9779423, … ┆ 9780460,  ┆           ┆   ┆ 0]        ┆ ,         ┆           ┆ … 23]     │\n",
       "│         ┆ 9779956]   ┆ …         ┆           ┆   ┆           ┆ 0.528305, ┆           ┆           │\n",
       "│         ┆            ┆ 9765777]  ┆           ┆   ┆           ┆ …         ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 0.508259] ┆           ┆           │\n",
       "│ 177925  ┆ [9777910,  ┆ [8560195, ┆ [9789922] ┆ … ┆ [0, 0, …  ┆ [0.520847 ┆ false     ┆ [3, 6, …  │\n",
       "│         ┆ 9777769, … ┆ 9486080,  ┆           ┆   ┆ 0]        ┆ ,         ┆           ┆ 1]        │\n",
       "│         ┆ 9779511]   ┆ …         ┆           ┆   ┆           ┆ 0.433627, ┆           ┆           │\n",
       "│         ┆            ┆ 9790091]  ┆           ┆   ┆           ┆ …         ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 0.570924] ┆           ┆           │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation = df_validation.with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the validation, simply add the testset to your flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 23282.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads\\predictions.txt to downloads\\predictions.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_submission_file(\n",
    "    impression_ids=df_validation[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_validation[\"ranked_scores\"],\n",
    "    path=\"downloads/predictions.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
